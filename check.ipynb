{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# dataset = np.load(\"//Users//jayanagaprakashthota//Downloads//nyc_taxi_data.npy\", allow_pickle=True).item()\n",
    "# X_train, y_train, X_test, y_test = dataset[\"X_train\"], dataset[\"y_train\"], dataset[\"X_test\"], dataset[\"y_test\"]\n",
    "\n",
    "# print(\"Training data shape:\")\n",
    "# print(\"X_train:\", X_train.shape)\n",
    "# print(\"y_train:\", y_train.shape)\n",
    "# print(\"Test data shape:\")\n",
    "# print(\"X_test:\", X_test.shape)\n",
    "# print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.random.randn(output_size)\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.dot(x, self.weights.T) + self.bias\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        grad_input = np.dot(grad, self.weights)\n",
    "        grad_weights = np.dot(grad.T, self.input)\n",
    "        grad_bias = np.sum(grad, axis=0)\n",
    "        return grad_input, grad_weights, grad_bias\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.output = 1 / (1 + np.exp(-x))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        grad_input = grad * self.output * (1 - self.output)\n",
    "        return grad_input\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        grad_input = grad.copy()\n",
    "        grad_input[self.mask] = 0\n",
    "        return grad_input\n",
    "\n",
    "class Sequential(Layer):\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "dataset = np.load(\"//Users//jayanagaprakashthota//Downloads//nyc_taxi_data.npy\", allow_pickle=True).item()\n",
    "X_train, y_train, X_test, y_test = dataset[\"X_train\"], dataset[\"y_train\"], dataset[\"X_test\"], dataset[\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, learning_rate=0.001, epochs=100, batch_size=128):\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data\n",
    "        indices = np.arange(len(X_train))\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(0, len(X_train_shuffled), batch_size):\n",
    "            X_batch = X_train_shuffled[i:i+batch_size]\n",
    "            y_batch = y_train_shuffled[i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model.forward(X_batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = np.mean((output - y_batch)**2)\n",
    "            train_loss_history.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            grad = 2 * (output - y_batch) / len(X_batch)\n",
    "            model.backward(grad)\n",
    "            \n",
    "            # Update weights\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, Linear):\n",
    "                    layer.weights -= learning_rate * layer.grad_weights\n",
    "                    layer.bias -= learning_rate * layer.grad_bias\n",
    "        \n",
    "        # Validate\n",
    "        val_output = model.forward(X_val)\n",
    "        val_loss = np.mean((val_output - y_val)**2)\n",
    "        val_loss_history.append(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch > 2 and val_loss_history[-1] >= val_loss_history[-2] >= val_loss_history[-3]:\n",
    "            break\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return train_loss_history, val_loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([ 630068,  907068, 1120451,  813547,  317666,  669205, 1028569, 1063512,\\n        620664,  664395,\\n       ...\\n       1001046,   48149,  255485, 1238917,  394567,  479028,  941570,  272089,\\n       1140720,  345300],\\n      dtype='int64', length=1312779)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model1\u001b[38;5;241m.\u001b[39madd(Sigmoid())\n\u001b[1;32m      7\u001b[0m model1\u001b[38;5;241m.\u001b[39madd(Linear(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m train_loss_history1, val_loss_history1 \u001b[38;5;241m=\u001b[39m train_model(model1, X_train, y_train, X_test, y_test)\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, X_train, y_train, X_val, y_val, learning_rate, epochs, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(X_train))\n\u001b[1;32m      7\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(indices)\n\u001b[0;32m----> 8\u001b[0m X_train_shuffled \u001b[38;5;241m=\u001b[39m X_train[indices]\n\u001b[1;32m      9\u001b[0m y_train_shuffled \u001b[38;5;241m=\u001b[39m y_train[indices]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Mini-batch training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([ 630068,  907068, 1120451,  813547,  317666,  669205, 1028569, 1063512,\\n        620664,  664395,\\n       ...\\n       1001046,   48149,  255485, 1238917,  394567,  479028,  941570,  272089,\\n       1140720,  345300],\\n      dtype='int64', length=1312779)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define and train the neural networks\n",
    "\n",
    "# Model 1: Linear -> Sigmoid -> Linear\n",
    "model1 = Sequential()\n",
    "model1.add(Linear(input_size=10, output_size=64))\n",
    "model1.add(Sigmoid())\n",
    "model1.add(Linear(input_size=64, output_size=1))\n",
    "\n",
    "train_loss_history1, val_loss_history1 = train_model(model1, X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Linear -> ReLU -> Linear\n",
    "model2 = Sequential()\n",
    "model2.add(Linear(input_size=10, output_size=64))\n",
    "model2.add(ReLU())\n",
    "model2.add(Linear(input_size=64, output_size=1))\n",
    "\n",
    "train_loss_history2, val_loss_history2 = train_model(model2, X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Linear -> Sigmoid -> ReLU -> Linear\n",
    "model3 = Sequential()\n",
    "model3.add(Linear(input_size=10, output_size=64))\n",
    "model3.add(Sigmoid())\n",
    "model3.add(ReLU())\n",
    "model3.add(Linear(input_size=64, output_size=1))\n",
    "\n",
    "train_loss_history3, val_loss_history3 = train_model(model3, X_train, y_train, X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
